use anyhow::{anyhow, Result};
use polars::prelude::*;
use pyo3::prelude::*;
use pyo3::types::PyDict;
use pyo3::{Bound, PyRefMut};
use pyo3_polars::PyDataFrame;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::process::Command;

/// Hiveè¿æ¥é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
#[pyclass]
pub struct HiveConfig {
    #[pyo3(get, set)]
    pub host: String,
    #[pyo3(get, set)]
    pub port: u16,
    #[pyo3(get, set)]
    pub username: String,
    #[pyo3(get, set)]
    pub database: String,
    #[pyo3(get, set)]
    pub auth: String,
}

#[pymethods]
impl HiveConfig {
    #[new]
    fn new(
        host: Option<String>,
        port: Option<u16>,
        username: Option<String>,
        database: Option<String>,
        auth: Option<String>,
    ) -> Self {
        Self {
            host: host.unwrap_or_else(|| "localhost".to_string()),
            port: port.unwrap_or(10000),
            username: username.unwrap_or_else(|| "default".to_string()),
            database: database.unwrap_or_else(|| "default".to_string()),
            auth: auth.unwrap_or_else(|| "NONE".to_string()),
        }
    }

    fn __repr__(&self) -> String {
        let host = &self.host;
        let port = self.port;
        let username = &self.username;
        let database = &self.database;
        let auth = &self.auth;
        format!("HiveConfig(host='{host}', port={port}, username='{username}', database='{database}', auth='{auth}')")
    }
}

/// Rustç‰ˆæœ¬çš„Hiveæ•°æ®è¯»å–å™¨
#[derive(Debug)]
#[pyclass]
pub struct RustHiveReader {
    config: HiveConfig,
    connected: bool,
}

#[pymethods]
impl RustHiveReader {
    #[new]
    fn new(config: Option<HiveConfig>) -> Self {
        let config = config.unwrap_or_else(|| HiveConfig::new(None, None, None, None, None));
        Self {
            config,
            connected: false,
        }
    }

    /// è¿æ¥åˆ°Hive
    fn connect(&mut self) -> PyResult<()> {
        let host = &self.config.host;
        let port = self.config.port;
        println!("ğŸ”— è¿æ¥åˆ°Hive: {host}:{port}");

        // è¿™é‡Œå®ç°å®é™…çš„è¿æ¥é€»è¾‘
        // ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿè¿æ¥æˆåŠŸ
        self.connected = true;
        println!("âœ… Hiveè¿æ¥æˆåŠŸ (Rustç‰ˆæœ¬)");
        Ok(())
    }

    /// æ–­å¼€è¿æ¥
    fn disconnect(&mut self) -> PyResult<()> {
        if self.connected {
            println!("ğŸ”Œ æ–­å¼€Hiveè¿æ¥");
            self.connected = false;
        }
        Ok(())
    }

    /// æ£€æŸ¥è¿æ¥çŠ¶æ€
    fn is_connected(&self) -> bool {
        self.connected
    }

    /// æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è¿”å›Polars DataFrame
    fn query_to_polars(&self, sql: String) -> PyResult<PyDataFrame> {
        if !self.connected {
            return Err(PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                "æœªè¿æ¥åˆ°Hiveï¼Œè¯·å…ˆè°ƒç”¨connect()",
            ));
        }

        let preview = &sql[..std::cmp::min(sql.len(), 50)];
        println!("ğŸ” æ‰§è¡ŒSQLæŸ¥è¯¢: {preview}");

        // è¿™é‡Œè°ƒç”¨å®é™…çš„æŸ¥è¯¢å®ç°
        let df = self
            .execute_sql_query(&sql)
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(e.to_string()))?;

        Ok(PyDataFrame(df))
    }

    /// æ˜¾ç¤ºæ‰€æœ‰è¡¨
    fn show_tables(&self) -> PyResult<PyDataFrame> {
        self.query_to_polars("SHOW TABLES".to_string())
    }

    /// æè¿°è¡¨ç»“æ„
    fn describe_table(&self, table_name: String) -> PyResult<PyDataFrame> {
        let sql = format!("DESCRIBE {table_name}");
        self.query_to_polars(sql)
    }

    /// è·å–è¡¨æ ·æœ¬æ•°æ®
    fn get_table_sample(&self, table_name: String, limit: Option<i32>) -> PyResult<PyDataFrame> {
        let limit = limit.unwrap_or(10);
        let sql = format!("SELECT * FROM {table_name} LIMIT {limit}");
        self.query_to_polars(sql)
    }

    /// è·å–é…ç½®ä¿¡æ¯
    fn get_config(&self) -> HiveConfig {
        self.config.clone()
    }
}

impl RustHiveReader {
    /// æ‰§è¡ŒSQLæŸ¥è¯¢çš„å†…éƒ¨å®ç°
    fn execute_sql_query(&self, sql: &str) -> Result<DataFrame> {
        // æ–¹æ¡ˆ1: ä½¿ç”¨beelineå‘½ä»¤è¡Œå®¢æˆ·ç«¯
        if std::env::var("USE_BEELINE").unwrap_or_default() == "true" {
            return self.execute_via_beeline(sql);
        }

        // æ–¹æ¡ˆ2: æ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•ï¼‰
        self.execute_mock_query(sql)
    }

    /// é€šè¿‡beelineæ‰§è¡ŒæŸ¥è¯¢
    fn execute_via_beeline(&self, sql: &str) -> Result<DataFrame> {
        let rt = tokio::runtime::Runtime::new()?;
        rt.block_on(async {
            let host = &self.config.host;
            let port = self.config.port;
            let database = &self.config.database;
            let jdbc_url = format!("jdbc:hive2://{host}:{port}/{database}");

            let output = Command::new("beeline")
                .args([
                    "-u",
                    &jdbc_url,
                    "-e",
                    sql,
                    "--outputformat=csv2",
                    "--silent=true",
                ])
                .output()
                .await?;

            if !output.status.success() {
                let error = String::from_utf8_lossy(&output.stderr);
                return Err(anyhow!("Beelineæ‰§è¡Œå¤±è´¥: {error}"));
            }

            let csv_data = String::from_utf8_lossy(&output.stdout);
            self.parse_csv_to_dataframe(&csv_data)
        })
    }

    /// æ¨¡æ‹ŸæŸ¥è¯¢æ‰§è¡Œï¼ˆç”¨äºæ¼”ç¤ºï¼‰
    fn execute_mock_query(&self, sql: &str) -> Result<DataFrame> {
        println!("ğŸ“Š æ¨¡æ‹Ÿæ‰§è¡ŒSQL: {sql}");

        // æ ¹æ®SQLç±»å‹è¿”å›ä¸åŒçš„æ¨¡æ‹Ÿæ•°æ®
        if sql.to_uppercase().contains("SHOW TABLES") {
            self.create_mock_tables_df()
        } else if sql.to_uppercase().contains("DESCRIBE") {
            self.create_mock_describe_df()
        } else if sql.to_uppercase().contains("SELECT") {
            self.create_mock_select_df(sql)
        } else {
            Err(anyhow!("ä¸æ”¯æŒçš„SQLç±»å‹"))
        }
    }

    /// åˆ›å»ºæ¨¡æ‹Ÿè¡¨åˆ—è¡¨
    fn create_mock_tables_df(&self) -> Result<DataFrame> {
        let tables = vec![
            "users".to_string(),
            "orders".to_string(),
            "products".to_string(),
            "analytics".to_string(),
            "logs".to_string(),
        ];

        let df = df! {
            "tab_name" => tables,
        }?;

        Ok(df)
    }

    /// åˆ›å»ºæ¨¡æ‹Ÿè¡¨ç»“æ„æè¿°
    fn create_mock_describe_df(&self) -> Result<DataFrame> {
        let df = df! {
            "col_name" => vec!["id", "name", "created_at", "status"],
            "data_type" => vec!["bigint", "string", "timestamp", "string"],
            "comment" => vec!["Primary key", "User name", "Creation time", "Record status"],
        }?;

        Ok(df)
    }

    /// åˆ›å»ºæ¨¡æ‹ŸæŸ¥è¯¢ç»“æœ
    fn create_mock_select_df(&self, sql: &str) -> Result<DataFrame> {
        // æ ¹æ®SQLç”Ÿæˆä¸åŒçš„æ¨¡æ‹Ÿæ•°æ®
        if sql.to_uppercase().contains("COUNT") {
            df! {
                "count" => vec![1000i64, 2000, 3000],
                "table_name" => vec!["table1", "table2", "table3"],
            }
            .map_err(|e| anyhow!("åˆ›å»ºDataFrameå¤±è´¥: {e}"))
        } else {
            // é€šç”¨çš„ç¤ºä¾‹æ•°æ®
            df! {
                "id" => vec![1i64, 2, 3, 4, 5],
                "name" => vec!["Alice", "Bob", "Charlie", "David", "Eve"],
                "score" => vec![95.5, 87.2, 92.1, 78.9, 88.7],
                "created_at" => vec![
                    "2025-01-01 10:00:00",
                    "2025-01-02 11:00:00",
                    "2025-01-03 12:00:00",
                    "2025-01-04 13:00:00",
                    "2025-01-05 14:00:00",
                ],
            }
            .map_err(|e| anyhow!("åˆ›å»ºDataFrameå¤±è´¥: {e}"))
        }
    }

    /// è§£æCSVæ•°æ®ä¸ºDataFrame
    fn parse_csv_to_dataframe(&self, csv_data: &str) -> Result<DataFrame> {
        // è¿™é‡Œå®ç°CSVè§£æé€»è¾‘
        // ä¸ºç®€å•èµ·è§ï¼Œè¿™é‡Œè¿”å›ä¸€ä¸ªç¤ºä¾‹DataFrame
        let df = df! {
            "data" => vec![csv_data],
        }?;
        Ok(df)
    }
}

/// ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ”¯æŒ
#[pyclass]
pub struct RustHiveContext {
    reader: RustHiveReader,
}

#[pymethods]
impl RustHiveContext {
    #[new]
    fn new(config: Option<HiveConfig>) -> Self {
        Self {
            reader: RustHiveReader::new(config),
        }
    }

    fn __enter__(mut slf: PyRefMut<'_, Self>) -> PyResult<PyRefMut<'_, Self>> {
        slf.reader.connect()?;
        Ok(slf)
    }

    fn __exit__(
        &mut self,
        _exc_type: Option<&Bound<'_, PyAny>>,
        _exc_value: Option<&Bound<'_, PyAny>>,
        _traceback: Option<&Bound<'_, PyAny>>,
    ) -> PyResult<bool> {
        self.reader.disconnect()?;
        Ok(false) // ä¸æŠ‘åˆ¶å¼‚å¸¸
    }

    // ä»£ç†æ–¹æ³•ï¼šè½¬å‘åˆ°å†…éƒ¨çš„RustHiveReader
    /// æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è¿”å›Polars DataFrame
    fn query_to_polars(&self, sql: String) -> PyResult<PyDataFrame> {
        self.reader.query_to_polars(sql)
    }

    /// æ˜¾ç¤ºæ‰€æœ‰è¡¨
    fn show_tables(&self) -> PyResult<PyDataFrame> {
        self.reader.show_tables()
    }

    /// æè¿°è¡¨ç»“æ„
    fn describe_table(&self, table_name: String) -> PyResult<PyDataFrame> {
        self.reader.describe_table(table_name)
    }

    /// è·å–è¡¨æ ·æœ¬æ•°æ®
    fn get_table_sample(&self, table_name: String, limit: Option<i32>) -> PyResult<PyDataFrame> {
        self.reader.get_table_sample(table_name, limit)
    }

    /// æ£€æŸ¥è¿æ¥çŠ¶æ€
    fn is_connected(&self) -> bool {
        self.reader.is_connected()
    }

    /// è·å–é…ç½®ä¿¡æ¯
    fn get_config(&self) -> HiveConfig {
        self.reader.get_config()
    }
}

/// ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºHiveé…ç½®
#[pyfunction]
fn create_hive_config(
    host: Option<String>,
    port: Option<u16>,
    username: Option<String>,
    database: Option<String>,
    auth: Option<String>,
) -> HiveConfig {
    HiveConfig::new(host, port, username, database, auth)
}

/// ä¾¿æ·å‡½æ•°ï¼šä»å­—å…¸åˆ›å»ºé…ç½®
#[pyfunction]
fn config_from_dict(config_dict: &Bound<'_, PyDict>) -> PyResult<HiveConfig> {
    let host = config_dict
        .get_item("host")?
        .map(|v| v.extract::<String>())
        .transpose()?;
    let port = config_dict
        .get_item("port")?
        .map(|v| v.extract::<u16>())
        .transpose()?;
    let username = config_dict
        .get_item("username")?
        .map(|v| v.extract::<String>())
        .transpose()?;
    let database = config_dict
        .get_item("database")?
        .map(|v| v.extract::<String>())
        .transpose()?;
    let auth = config_dict
        .get_item("auth")?
        .map(|v| v.extract::<String>())
        .transpose()?;

    Ok(HiveConfig::new(host, port, username, database, auth))
}

/// ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºé»˜è®¤é…ç½®
#[pyfunction]
fn create_default_config() -> HiveConfig {
    HiveConfig::new(None, None, None, None, None)
}

/// ä¾¿æ·å‡½æ•°ï¼šè·å–é»˜è®¤Hiveé…ç½®
#[pyfunction]
fn get_default_hive_config() -> HiveConfig {
    create_default_config()
}

/// ä¾¿æ·å‡½æ•°ï¼šè·å–é…ç½®ç®¡ç†å™¨ï¼ˆè¿”å›é…ç½®å¯¹è±¡ï¼‰
#[pyfunction]
fn get_config_manager() -> HiveConfig {
    create_default_config()
}

/// ä¾¿æ·å‡½æ•°ï¼šè¿æ¥åˆ°Hiveï¼ˆè¿”å›ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰
#[pyfunction]
fn connect_hive(config: Option<HiveConfig>) -> RustHiveContext {
    RustHiveContext::new(config)
}

/// æ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°
#[pyfunction]
fn benchmark_query(
    config: HiveConfig,
    sql: String,
    iterations: Option<usize>,
) -> PyResult<HashMap<String, f64>> {
    let iterations = iterations.unwrap_or(10);
    let mut reader = RustHiveReader::new(Some(config));
    reader.connect()?;

    let start = std::time::Instant::now();

    for _ in 0..iterations {
        let _result = reader.query_to_polars(sql.clone())?;
    }

    let duration = start.elapsed();
    let avg_time = duration.as_secs_f64() / iterations as f64;

    reader.disconnect()?;

    let mut results = HashMap::new();
    results.insert("total_time".to_string(), duration.as_secs_f64());
    results.insert("average_time".to_string(), avg_time);
    results.insert("iterations".to_string(), iterations as f64);
    results.insert(
        "queries_per_second".to_string(),
        iterations as f64 / duration.as_secs_f64(),
    );

    Ok(results)
}

/// Pythonæ¨¡å—å®šä¹‰
#[pymodule]
fn hive_reader_rs(_py: Python, m: &Bound<'_, PyModule>) -> PyResult<()> {
    // é…ç½®ç±»
    m.add_class::<HiveConfig>()?;

    // è¯»å–å™¨ç›¸å…³ç±»
    m.add_class::<RustHiveReader>()?;
    m.add_class::<RustHiveContext>()?;

    // å†™å…¥å™¨ç›¸å…³ç±»
    m.add_class::<WriteMode>()?;
    m.add_class::<RustHiveWriter>()?;
    m.add_class::<RustHiveWriteContext>()?;

    // å·¥å…·å‡½æ•°
    m.add_function(wrap_pyfunction!(create_hive_config, m)?)?;
    m.add_function(wrap_pyfunction!(config_from_dict, m)?)?;
    m.add_function(wrap_pyfunction!(create_default_config, m)?)?;
    m.add_function(wrap_pyfunction!(get_default_hive_config, m)?)?;
    m.add_function(wrap_pyfunction!(get_config_manager, m)?)?;
    m.add_function(wrap_pyfunction!(connect_hive, m)?)?;
    m.add_function(wrap_pyfunction!(connect_hive_writer, m)?)?;
    m.add_function(wrap_pyfunction!(benchmark_query, m)?)?;

    // ç‰ˆæœ¬ä¿¡æ¯
    m.add("__version__", "0.1.0")?;
    m.add("__author__", "Hive Reader & Writer Rust")?;

    Ok(())
}

/// å†™å…¥æ¨¡å¼æšä¸¾
#[derive(Debug, Clone, Serialize, Deserialize)]
#[pyclass]
pub enum WriteMode {
    #[pyo3(name = "overwrite")]
    Overwrite,
    #[pyo3(name = "append")]
    Append,
    #[pyo3(name = "error_if_exists")]
    ErrorIfExists,
    #[pyo3(name = "ignore")]
    Ignore,
}

#[pymethods]
impl WriteMode {
    fn __repr__(&self) -> String {
        match self {
            WriteMode::Overwrite => "WriteMode.Overwrite".to_string(),
            WriteMode::Append => "WriteMode.Append".to_string(),
            WriteMode::ErrorIfExists => "WriteMode.ErrorIfExists".to_string(),
            WriteMode::Ignore => "WriteMode.Ignore".to_string(),
        }
    }
}

/// Rustç‰ˆæœ¬çš„Hiveæ•°æ®å†™å…¥å™¨
#[derive(Debug)]
#[pyclass]
pub struct RustHiveWriter {
    config: HiveConfig,
    connected: bool,
}

#[pymethods]
impl RustHiveWriter {
    #[new]
    fn new(config: Option<HiveConfig>) -> Self {
        let config = config.unwrap_or_else(|| HiveConfig::new(None, None, None, None, None));
        Self {
            config,
            connected: false,
        }
    }

    /// è¿æ¥åˆ°Hive
    fn connect(&mut self) -> PyResult<()> {
        let host = &self.config.host;
        let port = self.config.port;
        println!("ğŸ”— è¿æ¥åˆ°Hiveå†™å…¥å™¨: {host}:{port}");

        // è¿™é‡Œå®ç°å®é™…çš„è¿æ¥é€»è¾‘
        self.connected = true;
        println!("âœ… Hiveå†™å…¥å™¨è¿æ¥æˆåŠŸ (Rustç‰ˆæœ¬)");
        Ok(())
    }

    /// æ–­å¼€è¿æ¥
    fn disconnect(&mut self) -> PyResult<()> {
        if self.connected {
            println!("ğŸ”Œ æ–­å¼€Hiveå†™å…¥å™¨è¿æ¥");
            self.connected = false;
        }
        Ok(())
    }

    /// æ£€æŸ¥è¿æ¥çŠ¶æ€
    fn is_connected(&self) -> bool {
        self.connected
    }

    /// å°†Polars DataFrameå†™å…¥Hiveè¡¨
    #[pyo3(signature = (df, table_name, mode = None, partition_cols = None, create_table = None))]
    fn write_table(
        &self,
        df: PyDataFrame,
        table_name: String,
        mode: Option<WriteMode>,
        partition_cols: Option<Vec<String>>,
        create_table: Option<bool>,
    ) -> PyResult<()> {
        if !self.connected {
            return Err(PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                "æœªè¿æ¥åˆ°Hiveï¼Œè¯·å…ˆè°ƒç”¨connect()",
            ));
        }

        let mode = mode.unwrap_or(WriteMode::ErrorIfExists);
        let create_table = create_table.unwrap_or(true);

        println!("ğŸ“ å†™å…¥æ•°æ®åˆ°è¡¨: {table_name}");

        // è°ƒç”¨å®é™…çš„å†™å…¥å®ç°
        self.execute_write_operation(&df.0, &table_name, &mode, &partition_cols, create_table)
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(e.to_string()))?;

        println!("âœ… æ•°æ®å†™å…¥å®Œæˆ");
        Ok(())
    }

    /// åˆ›å»ºè¡¨ç»“æ„
    fn create_table_from_dataframe(
        &self,
        df: PyDataFrame,
        table_name: String,
        partition_cols: Option<Vec<String>>,
    ) -> PyResult<()> {
        if !self.connected {
            return Err(PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                "æœªè¿æ¥åˆ°Hiveï¼Œè¯·å…ˆè°ƒç”¨connect()",
            ));
        }

        self.create_table_schema(&df.0, &table_name, &partition_cols)
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(e.to_string()))?;

        Ok(())
    }

    /// åˆ é™¤è¡¨
    fn drop_table(&self, table_name: String, if_exists: Option<bool>) -> PyResult<()> {
        if !self.connected {
            return Err(PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                "æœªè¿æ¥åˆ°Hiveï¼Œè¯·å…ˆè°ƒç”¨connect()",
            ));
        }

        let if_exists = if_exists.unwrap_or(false);
        let sql = if if_exists {
            format!("DROP TABLE IF EXISTS {table_name}")
        } else {
            format!("DROP TABLE {table_name}")
        };

        self.execute_ddl(&sql)
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(e.to_string()))?;

        println!("ğŸ—‘ï¸  è¡¨ {table_name} å·²åˆ é™¤");
        Ok(())
    }

    /// è·å–é…ç½®ä¿¡æ¯
    fn get_config(&self) -> HiveConfig {
        self.config.clone()
    }
}

impl RustHiveWriter {
    /// æ‰§è¡Œå†™å…¥æ“ä½œçš„å†…éƒ¨å®ç°
    fn execute_write_operation(
        &self,
        df: &DataFrame,
        table_name: &str,
        mode: &WriteMode,
        partition_cols: &Option<Vec<String>>,
        create_table: bool,
    ) -> Result<()> {
        // æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        let table_exists = self.check_table_exists(table_name)?;

        match mode {
            WriteMode::ErrorIfExists if table_exists => {
                return Err(anyhow!("è¡¨ {table_name} å·²å­˜åœ¨"));
            }
            WriteMode::Ignore if table_exists => {
                println!("âš ï¸  è¡¨ {table_name} å·²å­˜åœ¨ï¼Œå¿½ç•¥å†™å…¥");
                return Ok(());
            }
            WriteMode::Overwrite if table_exists => {
                println!("ğŸ”„ è¦†ç›–æ¨¡å¼ï¼šåˆ é™¤ç°æœ‰è¡¨ {table_name}");
                self.execute_ddl(&format!("DROP TABLE {table_name}"))?;
            }
            _ => {}
        }

        // å¦‚æœè¡¨ä¸å­˜åœ¨ä¸”éœ€è¦åˆ›å»ºè¡¨ï¼Œåˆ™åˆ›å»ºè¡¨ç»“æ„
        if (!table_exists || matches!(mode, WriteMode::Overwrite)) && create_table {
            self.create_table_schema(df, table_name, partition_cols)?;
        }

        // å†™å…¥æ•°æ®
        self.insert_dataframe_data(df, table_name, partition_cols)
    }

    /// æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    fn check_table_exists(&self, table_name: &str) -> Result<bool> {
        // æ–¹æ¡ˆ1: ä½¿ç”¨beelineå‘½ä»¤æ£€æŸ¥
        if std::env::var("USE_BEELINE").unwrap_or_default() == "true" {
            return self.check_table_exists_via_beeline(table_name);
        }

        // æ–¹æ¡ˆ2: æ¨¡æ‹Ÿæ£€æŸ¥ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
        println!("ğŸ” æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨: {table_name}");
        // è¿™é‡Œå¯ä»¥æ¨¡æ‹Ÿè¡¨å­˜åœ¨æ€§æ£€æŸ¥é€»è¾‘
        Ok(false) // é»˜è®¤å‡è®¾è¡¨ä¸å­˜åœ¨
    }

    /// é€šè¿‡beelineæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    fn check_table_exists_via_beeline(&self, table_name: &str) -> Result<bool> {
        let rt = tokio::runtime::Runtime::new()?;
        rt.block_on(async {
            let host = &self.config.host;
            let port = self.config.port;
            let database = &self.config.database;
            let jdbc_url = format!("jdbc:hive2://{host}:{port}/{database}");

            let sql = format!("SHOW TABLES LIKE '{table_name}'");

            let output = Command::new("beeline")
                .args([
                    "-u",
                    &jdbc_url,
                    "-e",
                    &sql,
                    "--outputformat=csv2",
                    "--silent=true",
                ])
                .output()
                .await?;

            if !output.status.success() {
                return Err(anyhow!("æ£€æŸ¥è¡¨å­˜åœ¨æ€§å¤±è´¥"));
            }

            let result = String::from_utf8_lossy(&output.stdout);
            Ok(!result.trim().is_empty())
        })
    }

    /// æ ¹æ®DataFrameåˆ›å»ºè¡¨ç»“æ„
    fn create_table_schema(
        &self,
        df: &DataFrame,
        table_name: &str,
        partition_cols: &Option<Vec<String>>,
    ) -> Result<()> {
        let schema = df.schema();
        let mut column_definitions = Vec::new();
        let mut partition_definitions = Vec::new();

        // æ„å»ºåˆ—å®šä¹‰
        for (name, dtype) in schema.iter() {
            let hive_type = self.polars_to_hive_type(dtype)?;
            let name_str = name.to_string(); // è½¬æ¢SmartStringåˆ°String

            if let Some(partitions) = partition_cols {
                if partitions.contains(&name_str) {
                    partition_definitions.push(format!("{name_str} {hive_type}"));
                    continue;
                }
            }

            column_definitions.push(format!("{name_str} {hive_type}"));
        }

        // æ„å»ºCREATE TABLEè¯­å¥
        let mut create_sql = format!(
            "CREATE TABLE {table_name} ({})",
            column_definitions.join(", ")
        );

        // æ·»åŠ åˆ†åŒºä¿¡æ¯
        if !partition_definitions.is_empty() {
            create_sql.push_str(&format!(
                " PARTITIONED BY ({})",
                partition_definitions.join(", ")
            ));
        }

        // æ·»åŠ å­˜å‚¨æ ¼å¼
        create_sql.push_str(" STORED AS PARQUET");

        println!("ğŸ—ï¸  åˆ›å»ºè¡¨ç»“æ„: {create_sql}");
        self.execute_ddl(&create_sql)
    }

    /// å°†Polarsæ•°æ®ç±»å‹è½¬æ¢ä¸ºHiveæ•°æ®ç±»å‹
    fn polars_to_hive_type(&self, dtype: &DataType) -> Result<String> {
        let hive_type = match dtype {
            DataType::Boolean => "BOOLEAN",
            DataType::Int8 | DataType::Int16 | DataType::Int32 => "INT",
            DataType::Int64 => "BIGINT",
            DataType::UInt8 | DataType::UInt16 | DataType::UInt32 => "INT",
            DataType::UInt64 => "BIGINT",
            DataType::Float32 => "FLOAT",
            DataType::Float64 => "DOUBLE",
            DataType::String => "STRING",
            DataType::Date => "DATE",
            DataType::Datetime(_, _) => "TIMESTAMP",
            _ => return Err(anyhow!("ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {:?}", dtype)),
        };
        Ok(hive_type.to_string())
    }

    /// æ‰§è¡ŒDDLè¯­å¥
    fn execute_ddl(&self, sql: &str) -> Result<()> {
        // æ–¹æ¡ˆ1: ä½¿ç”¨beelineæ‰§è¡ŒDDL
        if std::env::var("USE_BEELINE").unwrap_or_default() == "true" {
            return self.execute_ddl_via_beeline(sql);
        }

        // æ–¹æ¡ˆ2: æ¨¡æ‹Ÿæ‰§è¡Œï¼ˆç”¨äºæ¼”ç¤ºï¼‰
        println!("ğŸ“‹ æ‰§è¡ŒDDL: {sql}");
        Ok(())
    }

    /// é€šè¿‡beelineæ‰§è¡ŒDDL
    fn execute_ddl_via_beeline(&self, sql: &str) -> Result<()> {
        let rt = tokio::runtime::Runtime::new()?;
        rt.block_on(async {
            let host = &self.config.host;
            let port = self.config.port;
            let database = &self.config.database;
            let jdbc_url = format!("jdbc:hive2://{host}:{port}/{database}");

            let output = Command::new("beeline")
                .args(["-u", &jdbc_url, "-e", sql, "--silent=true"])
                .output()
                .await?;

            if !output.status.success() {
                let error = String::from_utf8_lossy(&output.stderr);
                return Err(anyhow!("DDLæ‰§è¡Œå¤±è´¥: {error}"));
            }

            Ok(())
        })
    }

    /// æ’å…¥DataFrameæ•°æ®
    fn insert_dataframe_data(
        &self,
        df: &DataFrame,
        table_name: &str,
        partition_cols: &Option<Vec<String>>,
    ) -> Result<()> {
        // æ–¹æ¡ˆ1: é€šè¿‡CSVæ–‡ä»¶å’ŒLOAD DATAæ–¹å¼
        if std::env::var("USE_CSV_LOAD").unwrap_or_default() == "true" {
            return self.insert_via_csv_load(df, table_name, partition_cols);
        }

        // æ–¹æ¡ˆ2: é€šè¿‡Parquetæ–‡ä»¶å’Œå¤–éƒ¨è¡¨æ–¹å¼
        if std::env::var("USE_PARQUET_LOAD").unwrap_or_default() == "true" {
            return self.insert_via_parquet_load(df, table_name, partition_cols);
        }

        // æ–¹æ¡ˆ3: ç”ŸæˆINSERTè¯­å¥ï¼ˆé€‚åˆå°æ•°æ®é‡ï¼‰
        self.insert_via_sql_statements(df, table_name, partition_cols)
    }

    /// é€šè¿‡CSVæ–‡ä»¶å’ŒLOAD DATAæ’å…¥æ•°æ®
    fn insert_via_csv_load(
        &self,
        df: &DataFrame,
        table_name: &str,
        _partition_cols: &Option<Vec<String>>,
    ) -> Result<()> {
        // åˆ›å»ºä¸´æ—¶CSVæ–‡ä»¶
        let temp_file = format!("/tmp/{table_name}_{}.csv", chrono::Utc::now().timestamp());

        // ä½¿ç”¨LazyFrameå†™å…¥CSVæ–‡ä»¶ (é¿å…å¯å˜å¼•ç”¨é—®é¢˜)
        let mut df_clone = df.clone();
        let mut file = std::fs::File::create(&temp_file)?;
        CsvWriter::new(&mut file)
            .include_header(false)
            .finish(&mut df_clone)?;

        // æ‰§è¡ŒLOAD DATAè¯­å¥
        let load_sql = format!("LOAD DATA LOCAL INPATH '{temp_file}' INTO TABLE {table_name}");

        self.execute_ddl(&load_sql)?;

        // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        let _ = std::fs::remove_file(&temp_file);

        Ok(())
    }

    /// é€šè¿‡Parquetæ–‡ä»¶æ’å…¥æ•°æ® (ç®€åŒ–ç‰ˆæœ¬)
    fn insert_via_parquet_load(
        &self,
        _df: &DataFrame,
        table_name: &str,
        _partition_cols: &Option<Vec<String>>,
    ) -> Result<()> {
        // åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„
        let temp_file = format!(
            "/tmp/{table_name}_{}.parquet",
            chrono::Utc::now().timestamp()
        );

        println!("ğŸ“¦ å°†ç”ŸæˆParquetæ–‡ä»¶: {temp_file}");
        println!("ğŸ“‹ è¯·ä½¿ç”¨å¤–éƒ¨å·¥å…·å°†DataFrameä¿å­˜ä¸ºParquetå¹¶ä¸Šä¼ åˆ°HDFS");
        println!("ğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ df.write_parquet() æ–¹æ³•ä¿å­˜æ–‡ä»¶");

        // è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªåŠ¨ä¸Šä¼ åˆ°HDFSçš„é€»è¾‘
        // ç”±äºParquetWriterçš„APIé—®é¢˜ï¼Œæš‚æ—¶ä½¿ç”¨æç¤ºä¿¡æ¯

        Ok(())
    }

    /// é€šè¿‡INSERTè¯­å¥æ’å…¥æ•°æ®ï¼ˆé€‚åˆå°æ•°æ®é‡ï¼‰
    fn insert_via_sql_statements(
        &self,
        df: &DataFrame,
        table_name: &str,
        _partition_cols: &Option<Vec<String>>,
    ) -> Result<()> {
        let rows_count = df.height();
        if rows_count > 1000 {
            println!("âš ï¸  æ•°æ®é‡è¾ƒå¤§({rows_count}è¡Œ)ï¼Œå»ºè®®ä½¿ç”¨CSVæˆ–Parquetæ–¹å¼å¯¼å…¥");
        }

        // æ„å»ºINSERTè¯­å¥
        let columns: Vec<String> = df
            .get_column_names()
            .iter()
            .map(|s| s.to_string())
            .collect();
        let column_list = columns.join(", ");

        // æ‰¹é‡æ’å…¥æ•°æ®
        let batch_size = 100;
        for chunk_start in (0..rows_count).step_by(batch_size) {
            let chunk_end = std::cmp::min(chunk_start + batch_size, rows_count);
            let chunk_df = df.slice(chunk_start as i64, chunk_end - chunk_start);

            let values = self.dataframe_to_values_string(&chunk_df)?;
            let insert_sql = format!("INSERT INTO {table_name} ({column_list}) VALUES {values}");

            self.execute_ddl(&insert_sql)?;
        }

        Ok(())
    }

    /// å°†DataFrameè½¬æ¢ä¸ºVALUESå­—ç¬¦ä¸²
    fn dataframe_to_values_string(&self, df: &DataFrame) -> Result<String> {
        let mut values = Vec::new();

        for row_idx in 0..df.height() {
            let mut row_values = Vec::new();

            for column in df.get_columns() {
                let value = self.format_column_value(column, row_idx)?;
                row_values.push(value);
            }

            values.push(format!("({})", row_values.join(", ")));
        }

        Ok(values.join(", "))
    }

    /// æ ¼å¼åŒ–åˆ—å€¼
    fn format_column_value(&self, column: &Series, row_idx: usize) -> Result<String> {
        let value = column.get(row_idx)?;
        let formatted = match value {
            AnyValue::Null => "NULL".to_string(),
            AnyValue::Boolean(b) => b.to_string(),
            AnyValue::Int8(i) => i.to_string(),
            AnyValue::Int16(i) => i.to_string(),
            AnyValue::Int32(i) => i.to_string(),
            AnyValue::Int64(i) => i.to_string(),
            AnyValue::UInt8(i) => i.to_string(),
            AnyValue::UInt16(i) => i.to_string(),
            AnyValue::UInt32(i) => i.to_string(),
            AnyValue::UInt64(i) => i.to_string(),
            AnyValue::Float32(f) => f.to_string(),
            AnyValue::Float64(f) => f.to_string(),
            _ => {
                // å¤„ç†å­—ç¬¦ä¸²å’Œå…¶ä»–ç±»å‹ï¼Œç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                let str_value = format!("{value}");
                if str_value.contains('"') || str_value.contains('\'') {
                    let escaped_value = str_value.replace('\'', "''");
                    format!("'{escaped_value}'")
                } else {
                    format!("'{str_value}'")
                }
            }
        };
        Ok(formatted)
    }
}

/// Hiveå†™å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
#[derive(Debug)]
#[pyclass]
pub struct RustHiveWriteContext {
    writer: RustHiveWriter,
}

#[pymethods]
impl RustHiveWriteContext {
    #[new]
    fn new(config: Option<HiveConfig>) -> Self {
        Self {
            writer: RustHiveWriter::new(config),
        }
    }

    fn __enter__(mut slf: PyRefMut<'_, Self>) -> PyResult<PyRefMut<'_, Self>> {
        slf.writer.connect()?;
        Ok(slf)
    }

    fn __exit__(
        &mut self,
        _exc_type: Option<&Bound<'_, PyAny>>,
        _exc_value: Option<&Bound<'_, PyAny>>,
        _traceback: Option<&Bound<'_, PyAny>>,
    ) -> PyResult<bool> {
        if let Err(e) = self.writer.disconnect() {
            eprintln!("è­¦å‘Š: æ–­å¼€è¿æ¥æ—¶å‡ºé”™: {e}");
        }
        Ok(false)
    }

    /// å†™å…¥è¡¨
    #[pyo3(signature = (df, table_name, mode = None, partition_cols = None, create_table = None))]
    fn write_table(
        &self,
        df: PyDataFrame,
        table_name: String,
        mode: Option<WriteMode>,
        partition_cols: Option<Vec<String>>,
        create_table: Option<bool>,
    ) -> PyResult<()> {
        self.writer
            .write_table(df, table_name, mode, partition_cols, create_table)
    }

    /// åˆ›å»ºè¡¨
    fn create_table_from_dataframe(
        &self,
        df: PyDataFrame,
        table_name: String,
        partition_cols: Option<Vec<String>>,
    ) -> PyResult<()> {
        self.writer
            .create_table_from_dataframe(df, table_name, partition_cols)
    }

    /// åˆ é™¤è¡¨
    fn drop_table(&self, table_name: String, if_exists: Option<bool>) -> PyResult<()> {
        self.writer.drop_table(table_name, if_exists)
    }

    /// æ£€æŸ¥è¿æ¥çŠ¶æ€
    fn is_connected(&self) -> bool {
        self.writer.is_connected()
    }

    /// è·å–é…ç½®ä¿¡æ¯
    fn get_config(&self) -> HiveConfig {
        self.writer.get_config()
    }
}

/// ä¾¿æ·çš„å†™å…¥è¿æ¥å‡½æ•°
#[pyfunction]
fn connect_hive_writer(config: Option<HiveConfig>) -> RustHiveWriteContext {
    RustHiveWriteContext::new(config)
}
